{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pickle \n",
    "import numpy as np\n",
    "import keras.utils\n",
    "import time\n",
    "from keras.callbacks import TensorBoard, CSVLogger\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,LSTM,Conv1D,GlobalMaxPool1D,Dropout,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "from nltk.corpus import stopwords\n",
    "import operator,re\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\13051\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# #!python -m spacy download en_core_web_sm\n",
    "# nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'D:/train.csv' does not exist: b'D:/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5b9b88a2091b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'user_verified'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_description'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_gender'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_messages'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_followers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_location'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_friends'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'has_url'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comments'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pics'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'likes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reposts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rumor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'multi_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/valid.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'user_verified'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_description'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_gender'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_messages'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_followers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_location'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_friends'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'has_url'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comments'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pics'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'likes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reposts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rumor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'multi_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'user_verified'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_description'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_gender'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_messages'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_followers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_location'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_friends'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'has_url'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'comments'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pics'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'likes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reposts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rumor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'multi_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'D:/train.csv' does not exist: b'D:/train.csv'"
     ]
    }
   ],
   "source": [
    "train_data1 = pd.read_csv('./train.csv', encoding='UTF-8')[['multi','text','user_verified', 'user_description', 'user_gender', 'user_messages', 'user_followers', 'user_location', 'user_time', 'user_friends', 'has_url', 'comments', 'pics', 'likes', 'time', 'reposts', 'rumor','multi_type','user']]\n",
    "val_data1 = pd.read_csv('./valid.csv',encoding='UTF-8')[['multi','text','user_verified', 'user_description', 'user_gender', 'user_messages', 'user_followers', 'user_location', 'user_time', 'user_friends', 'has_url', 'comments', 'pics', 'likes', 'time', 'reposts', 'rumor','multi_type','user']]\n",
    "test_data1 = pd.read_csv('./test.csv', encoding='UTF-8')[['multi','text','user_verified', 'user_description', 'user_gender', 'user_messages', 'user_followers', 'user_location', 'user_time', 'user_friends', 'has_url', 'comments', 'pics', 'likes', 'time', 'reposts', 'rumor','multi_type','user']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "train_data=train_data1[~train_data1['user'].isin(['empty'])] #删除user=\"empty\"的行\n",
    "val_data=val_data1[~val_data1['user'].isin(['empty'])]\n",
    "test_data=test_data1[~test_data1['user'].isin(['empty'])] \n",
    "\n",
    "train_data.loc[train_data['multi'].isnull(),'multi']=train_data[train_data['multi'].isnull()]['multi_type'] #当multi为空时用multi_type替代\n",
    "val_data.loc[val_data['multi'].isnull(),'multi']=val_data[val_data['multi'].isnull()]['multi_type'] \n",
    "test_data.loc[test_data['multi'].isnull(),'multi']=test_data[test_data['multi'].isnull()]['multi_type'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 1, 'no': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "### OUTPUT LABELS ###\n",
    "#####################\n",
    "\n",
    "y_label_dict = {\"yes\" : 1, \"no\" : 0}\n",
    "print(y_label_dict)\n",
    "\n",
    "train_data['rumor_id'] = train_data['rumor'].apply(lambda x: y_label_dict[x])\n",
    "val_data['rumor_id'] = val_data['rumor'].apply(lambda x: y_label_dict[x])\n",
    "test_data['rumor_id'] = test_data['rumor'].apply(lambda x: y_label_dict[x])\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 1, 'f': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### USER_VERIFIED_DICT ###\n",
    "########################\n",
    "\n",
    "user_verified_dict = {\"t\" : 1, \"f\" : 0}\n",
    "print(user_verified_dict)\n",
    "\n",
    "train_data['user_verified_id'] = train_data['user_verified'].apply(lambda x: user_verified_dict[x])\n",
    "val_data['user_verified_id'] = val_data['user_verified'].apply(lambda x: user_verified_dict[x])\n",
    "test_data['user_verified_id'] = test_data['user_verified'].apply(lambda x: user_verified_dict[x])\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 1, 'f': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### USER_DISCRIPTION_DICT ###\n",
    "###########################\n",
    "\n",
    "user_description_dict = {\"t\" : 1, \"f\" : 0}\n",
    "print(user_description_dict)\n",
    "\n",
    "train_data['user_description_id'] = train_data['user_description'].apply(lambda x: user_description_dict[x])\n",
    "val_data['user_description_id'] = val_data['user_description'].apply(lambda x: user_description_dict[x])\n",
    "test_data['user_description_id'] = test_data['user_description'].apply(lambda x: user_description_dict[x])\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 1, 'm': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### USER_GENDER_DICT ###\n",
    "###########################\n",
    "\n",
    "user_gender_dict = {\"f\" : 1, \"m\" : 0}\n",
    "print(user_gender_dict)\n",
    "\n",
    "train_data['user_gender_id'] = train_data['user_gender'].apply(lambda x: user_gender_dict[x])\n",
    "val_data['user_gender_id'] = val_data['user_gender'].apply(lambda x: user_gender_dict[x])\n",
    "test_data['user_gender_id'] = test_data['user_gender'].apply(lambda x: user_gender_dict[x])\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    118036.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "### USER_MESSAGE_DICT ###\n",
    "#########################\n",
    "print(train_data['user_messages'].mode())\n",
    "bins=[0,]\n",
    "# 1947 max=335237  min=0 median=11552  mean=28219.43\n",
    "# user_message_dict = {\"f\" : 1, \"m\" : 0}\n",
    "# print(user_gender_dict)\n",
    "\n",
    "# train_data['user_gender_id'] = train_data['user_gender'].apply(lambda x: user_gender_dict[x])\n",
    "# val_data['user_gender_id'] = val_data['user_gender'].apply(lambda x: user_gender_dict[x])\n",
    "# test_data['user_gender_id'] = test_data['user_gender'].apply(lambda x: user_gender_dict[x])\n",
    "\n",
    "# num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 1, 'f': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### HAS_URL_DICT ###\n",
    "###########################\n",
    "\n",
    "has_url_dict = {\"t\" : 1, \"f\" : 0}\n",
    "print(has_url_dict)\n",
    "\n",
    "train_data['has_url_id'] = train_data['has_url'].apply(lambda x: has_url_dict[x])\n",
    "val_data['has_url_id'] = val_data['has_url'].apply(lambda x: has_url_dict[x])\n",
    "test_data['has_url_id'] = test_data['has_url'].apply(lambda x: has_url_dict[x])\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2     628\n",
       "5     320\n",
       "24    194\n",
       "9     154\n",
       "20    86 \n",
       "32    67 \n",
       "16    62 \n",
       "29    48 \n",
       "25    46 \n",
       "3     40 \n",
       "14    34 \n",
       "26    33 \n",
       "21    32 \n",
       "13    28 \n",
       "33    19 \n",
       "11    18 \n",
       "0     17 \n",
       "18    15 \n",
       "23    14 \n",
       "27    12 \n",
       "10    9  \n",
       "6     9  \n",
       "17    8  \n",
       "8     8  \n",
       "22    7  \n",
       "12    7  \n",
       "31    7  \n",
       "7     5  \n",
       "4     5  \n",
       "30    5  \n",
       "15    4  \n",
       "19    2  \n",
       "28    2  \n",
       "1     2  \n",
       "Name: location_id, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "###   USER_LOCATION    ###\n",
    "########################\n",
    "\n",
    "#frequent_speakers = train_data['speaker'].value_counts()[:20].reset_index().to_dict()['index']\n",
    "#frequent_speakers = dict((v,k) for k,v in frequent_speakers.iteritems())\n",
    "\n",
    "frequent_location = {'安徽' : 0, '澳门' : 1, '北京' : 2, '福建' : 3, '甘肃' : 4, '广东' : 5, \n",
    "                     '广西' : 6, '贵州' : 7, '海南': 8, '海外':9, '河北':10, '河南':11, '黑龙':12, \n",
    "                     '湖北' : 13, '湖南': 14, '吉林' : 15, '江苏':16,'江西':17, '辽宁':18,'内蒙' : 19, \n",
    "                     '其他' : 20,'山东':21, '山西' : 22, '陕西' : 23, '上海' : 24, '四川':25,'台湾':26, \n",
    "                     '天津':27, '西藏':28, '香港':29, '新疆':30, '云南':31, '浙江':32, '重庆':33, \n",
    "                     '青海':34, '宁夏':35}\n",
    "#print(frequent_location)\n",
    "\n",
    "key_ls=frequent_location.keys()\n",
    "def get_location_id(location):\n",
    "    if isinstance(location, str):\n",
    "        if location[:2] in key_ls:\n",
    "            return frequent_location[location[:2]]\n",
    "        else:\n",
    "            return len(set(frequent_location.values())) \n",
    "    else:\n",
    "        return len(set(frequent_location.values())) \n",
    "\n",
    "# key_ls=frequent_location.keys()\n",
    "# def get_location_id(location):\n",
    "#     if location[:2] in key_ls:\n",
    "#         return len(frequent_location.values())\n",
    "\n",
    "train_data['location_id'] = train_data['user_location'].apply(get_location_id)\n",
    "val_data['location_id'] = val_data['user_location'].apply(get_location_id)\n",
    "test_data['location_id'] = test_data['user_location'].apply(get_location_id)\n",
    "\n",
    "print(len(set(frequent_location.values())))\n",
    "\n",
    "train_data['location_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\180773~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.673 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18124\n",
      "Created Vocabulary Dictionary...\n",
      "Saved Vocabulary Dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pro\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "###   TEXT   ###\n",
    "##############\n",
    "\n",
    "stopwords = [line.strip() for line in open('D:/stopwordS.txt', 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "def proc_text(text):\n",
    "    filter_pattern = re.compile('[^\\u4E00-\\u9FA5]+')\n",
    "    chinese_only = filter_pattern.sub('', text)\n",
    "    words_lst = pseg.cut(chinese_only)\n",
    "    meaninful_words = []\n",
    "    for word, flag in words_lst:\n",
    "#      if (word not in stopwords) and (flag == 'v'):\n",
    "            # 也可根据词性去除非动词等\n",
    "    # handle stopwords:\n",
    "        if word not in stopwords :   #len(word) >= 2 and \n",
    "            meaninful_words.append(word)\n",
    "    return ' '.join(meaninful_words)\n",
    "\n",
    "def load_statement_vocab_dict(train_data):\n",
    "    vocabulary_dict = {}\n",
    "    if not os.path.exists('vocabulary.p'):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(train_data)\n",
    "        vocabulary_dict = tokenizer.word_index\n",
    "        print(len(vocabulary_dict))\n",
    "        pickle.dump(vocabulary_dict, open( \"vocabulary.p\", \"wb\" ))\n",
    "        print('Created Vocabulary Dictionary...')\n",
    "        print('Saved Vocabulary Dictionary...')\n",
    "    else:\n",
    "        print('Loading Vocabulary Dictionary...')\n",
    "        vocabulary_dict = pickle.load(open(\"vocabulary.p\", \"rb\" ))\n",
    "    return vocabulary_dict\n",
    "\n",
    "\n",
    "def preprocess_statement(text):\n",
    "    val = [0] * 10\n",
    "    val = [vocabulary_dict[t] for t in text if t in vocabulary_dict] \n",
    "    return val\n",
    "\n",
    "\n",
    "train_pre = train_data['text'].apply(proc_text)\n",
    "val_pre = val_data['text'].apply(proc_text)\n",
    "test_pre = test_data['text'].apply(proc_text)\n",
    "\n",
    "vocabulary_dict = load_statement_vocab_dict(train_pre)\n",
    "\n",
    "train_data['word_id'] = train_pre.apply(preprocess_statement)\n",
    "val_data['word_id'] = val_pre.apply(preprocess_statement)\n",
    "test_data['word_id'] = test_pre.apply(preprocess_statement)\n",
    "\n",
    "# 过滤空字符串\n",
    "# train_data = train_data[train_data['text'] != '']\n",
    "# val_data = val_data[val_data['text'] != '']\n",
    "# test_data = test_data[test_data['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189595  : Word Embeddings Found\n",
      "300  : Embedding Dimension\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "###  EMBEDDINGS   ###\n",
    "#####################\n",
    "\n",
    "embeddings = {}\n",
    "with open(\"D:/sgns.weibo.txt\", encoding='utf-8', errors='ignore') as file_object:\n",
    "  for line in file_object:\n",
    "    word_embed = line.split()\n",
    "    word = word_embed[0]\n",
    "    embed = np.array(word_embed[1:])\n",
    "    embeddings[word.lower()]= embed\n",
    "\n",
    "EMBED_DIM = 300\n",
    "print(len(embeddings), \" : Word Embeddings Found\")\n",
    "print(len(embeddings[word]), \" : Embedding Dimension\")\n",
    "\n",
    "\n",
    "num_words = len(vocabulary_dict) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBED_DIM))\n",
    "for word, i in vocabulary_dict.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embeddings_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.912511,  0.15435 ,  0.519588, ..., -0.536558, -0.357079,\n",
       "         0.75555 ],\n",
       "       [ 0.65388 ,  0.756262,  0.659758, ...,  0.008703, -0.100621,\n",
       "         0.610115],\n",
       "       ...,\n",
       "       [-0.282047, -0.011514,  0.681716, ..., -0.113365,  0.486522,\n",
       "        -0.553472],\n",
       "       [ 0.915986,  0.242119,  0.061393, ..., -0.32598 ,  0.160503,\n",
       "        -0.034218],\n",
       "       [ 0.380722, -0.26947 ,  0.631396, ..., -1.495936, -0.052521,\n",
       "        -0.033431]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "36\n",
      "2\n",
      "Index(['multi', 'text', 'user_verified', 'user_description', 'user_gender',\n",
      "       'user_messages', 'user_followers', 'user_location', 'user_time',\n",
      "       'user_friends', 'has_url', 'comments', 'pics', 'likes', 'time',\n",
      "       'reposts', 'rumor', 'multi_type', 'user', 'rumor_id',\n",
      "       'user_verified_id', 'user_description_id', 'user_gender_id',\n",
      "       'has_url_id', 'location_id', 'word_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "###  HYPERPARAMS  ###\n",
    "#####################\n",
    "vocab_length = len(vocabulary_dict.keys())\n",
    "hidden_size = EMBED_DIM #Has to be same as EMBED_DIM\n",
    "lstm_size = 100\n",
    "num_steps = 35\n",
    "num_epochs = 30\n",
    "batch_size = 64     #批量大小，决定一次训练的样本数目，batchsize 的正确选择是为了在内存效率和内存容量之间寻找最佳平衡\n",
    "#batch_size为一个batch中样本的数量（当成一个batch的行），num_steps是一个样本的序列长度（当成一个batch的列）\n",
    "#num_steps 表示一次有15个单词输入给相连的15个lstm神经元\n",
    "#batch_size 表示一个迭代里有40组这样的15个单词的样本\n",
    "#one epoch = numbers of iterations = N = 训练样本的数量/batch_size\n",
    "\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [3,3,3]\n",
    "filter_size = 128\n",
    "\n",
    "#Meta data related hyper params\n",
    "num_verified = len(train_data.user_verified_id.unique())\n",
    "num_description = len(train_data.user_description_id.unique())\n",
    "num_gender = len(train_data.user_gender_id.unique())\n",
    "num_location = len(train_data.location_id.unique())+2\n",
    "num_has_url = len(train_data.has_url_id.unique())\n",
    "\n",
    "print(num_verified)\n",
    "print(num_description)\n",
    "print(num_gender)\n",
    "print(num_location)\n",
    "print(num_has_url)\n",
    "\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "###   DATA PREP   ###\n",
    "#####################\n",
    "\n",
    "# word_id - statement\n",
    "X_train = train_data['word_id']\n",
    "X_val = val_data['word_id']\n",
    "X_test = test_data['word_id']\n",
    "\n",
    "# ouput - label\n",
    "Y_train = train_data['rumor_id']\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=2)\n",
    "\n",
    "Y_val = val_data['rumor_id']\n",
    "Y_val = keras.utils.to_categorical(Y_val, num_classes=2)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=num_steps, padding='post',truncating='post')\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=num_steps, padding='post',truncating='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=num_steps, padding='post',truncating='post')\n",
    "\n",
    "\n",
    "#Meta data preparation\n",
    "verified_train = keras.utils.to_categorical(train_data['user_verified_id'], num_classes=num_verified)\n",
    "description_train = keras.utils.to_categorical(train_data['user_description_id'], num_classes=num_description)\n",
    "gender_train = keras.utils.to_categorical(train_data['user_gender_id'], num_classes=num_gender)\n",
    "location_train = keras.utils.to_categorical(train_data['location_id'], num_classes=num_location)\n",
    "has_url_train = keras.utils.to_categorical(train_data['has_url_id'], num_classes=num_has_url)\n",
    "\n",
    "# X_train_meta = location_train\n",
    "X_train_meta = np.hstack((verified_train, description_train, gender_train, location_train, has_url_train))\n",
    "\n",
    "verified_val = keras.utils.to_categorical(val_data['user_verified_id'], num_classes=num_verified)\n",
    "description_val = keras.utils.to_categorical(val_data['user_description_id'], num_classes=num_description)\n",
    "gender_val = keras.utils.to_categorical(val_data['user_gender_id'], num_classes=num_gender)\n",
    "location_val = keras.utils.to_categorical(val_data['location_id'], num_classes=num_location)\n",
    "has_url_val = keras.utils.to_categorical(val_data['has_url_id'], num_classes=num_has_url)\n",
    "\n",
    "#X_val_meta = location_val\n",
    "X_val_meta = np.hstack((verified_val, description_val, gender_val, location_val, has_url_val))\n",
    "\n",
    "verified_test = keras.utils.to_categorical(test_data['user_verified_id'], num_classes=num_verified)\n",
    "description_test = keras.utils.to_categorical(test_data['user_description_id'], num_classes=num_description)\n",
    "gender_test = keras.utils.to_categorical(test_data['user_gender_id'], num_classes=num_gender)\n",
    "location_test = keras.utils.to_categorical(test_data['location_id'], num_classes=num_location)\n",
    "has_url_test = keras.utils.to_categorical(test_data['has_url_id'], num_classes=num_has_url)\n",
    "\n",
    "#X_test_meta = location_test\n",
    "X_test_meta = np.hstack((verified_test, description_test, gender_test, location_test, has_url_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1947, 44) (678, 44) (674, 44)\n",
      "(1947, 35) (678, 35) (674, 35)\n",
      "(1947, 2) (678, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_meta.shape, X_val_meta.shape, X_test_meta.shape)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, name, use_pos = False, use_meta = False, use_dep = False):\n",
    "    sgd = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    adam = optimizers.Adam(lr=0.025, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "    # optimizer制定优化方式，loss制定损失函数，metrics指定衡量模型的指标\n",
    "    # “categorical_crossentropy”多分类的对数损失函数，与softmax分类器相对应的损失函数\n",
    "    #　tip：此损失函数与上一类同属对数损失函数，sigmoid和softmax的区别主要是，sigmoid用于二分类，softmax用于多分类\n",
    "    tb = TensorBoard()\n",
    "    csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "    filepath= name+\"_weights_best.hdf5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "  \n",
    "  \n",
    "    model.fit(\n",
    "        {'main_input': X_train},\n",
    "        {'main_output': Y_train}, epochs = num_epochs, batch_size = batch_size,\n",
    "        validation_data = (\n",
    "            {'main_input': X_val},\n",
    "            {'main_output': Y_val}\n",
    "        ), callbacks=[tb,csv_logger,checkpoint])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\pro\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\pro\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "kernel_stmt = []\n",
    "kernel_pos = []\n",
    "kernel_dep = []\n",
    "\n",
    "use_pos = False\n",
    "use_meta = False\n",
    "use_dep = False\n",
    "\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x_stmt = Embedding(vocab_length+1,EMBED_DIM,weights=[embedding_matrix],input_length=num_steps,trainable=False)(statement_input) \n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel)(x_stmt)\n",
    "    x_1 = GlobalMaxPool1D()(x_1)\n",
    "    kernel_stmt.append(x_1)\n",
    "    \n",
    "\n",
    "    \n",
    "conv_in1 = keras.layers.concatenate(kernel_stmt)\n",
    "conv_in1 = Dropout(0.7)(conv_in1)\n",
    "conv_in1 = Dense(128, activation='relu')(conv_in1)\n",
    "\n",
    "\n",
    "\n",
    "#meta data\n",
    "meta_input = Input(shape=(X_train_meta.shape[1],), name='aux_input')\n",
    "x_meta = Dense(64, activation='relu')(meta_input)\n",
    "\n",
    "# x = keras.layers.concatenate([conv_in1, x_meta])    #text+meta\n",
    "x = conv_in1     #text\n",
    "\n",
    "main_output = Dense(2, activation='softmax', name='main_output')(x)\n",
    "# model_cnn = Model(inputs=[statement_input, meta_input], outputs=[main_output])     #text+meta\n",
    "model_cnn = Model(inputs=[statement_input], outputs=[main_output])   #text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\pro\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\pro\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1947 samples, validate on 678 samples\n",
      "Epoch 1/30\n",
      "1947/1947 [==============================] - 2s 1ms/step - loss: 0.9649 - categorical_accuracy: 0.5177 - val_loss: 0.6525 - val_categorical_accuracy: 0.5885\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.58850, saving model to cnn_weights_best.hdf5\n",
      "Epoch 2/30\n",
      "1947/1947 [==============================] - 1s 762us/step - loss: 0.6821 - categorical_accuracy: 0.6009 - val_loss: 0.6233 - val_categorical_accuracy: 0.6136\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.58850 to 0.61357, saving model to cnn_weights_best.hdf5\n",
      "Epoch 3/30\n",
      "1947/1947 [==============================] - 1s 742us/step - loss: 0.5897 - categorical_accuracy: 0.6893 - val_loss: 0.5206 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.61357 to 0.77729, saving model to cnn_weights_best.hdf5\n",
      "Epoch 4/30\n",
      "1947/1947 [==============================] - 1s 733us/step - loss: 0.4967 - categorical_accuracy: 0.7648 - val_loss: 0.4328 - val_categorical_accuracy: 0.8112\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.77729 to 0.81121, saving model to cnn_weights_best.hdf5\n",
      "Epoch 5/30\n",
      "1947/1947 [==============================] - 1s 726us/step - loss: 0.4002 - categorical_accuracy: 0.8218 - val_loss: 0.3911 - val_categorical_accuracy: 0.8171\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.81121 to 0.81711, saving model to cnn_weights_best.hdf5\n",
      "Epoch 6/30\n",
      "1947/1947 [==============================] - 1s 728us/step - loss: 0.3268 - categorical_accuracy: 0.8536 - val_loss: 0.3521 - val_categorical_accuracy: 0.8378\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.81711 to 0.83776, saving model to cnn_weights_best.hdf5\n",
      "Epoch 7/30\n",
      "1947/1947 [==============================] - 1s 740us/step - loss: 0.2331 - categorical_accuracy: 0.9111 - val_loss: 0.3932 - val_categorical_accuracy: 0.8171\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.83776\n",
      "Epoch 8/30\n",
      "1947/1947 [==============================] - 1s 728us/step - loss: 0.2022 - categorical_accuracy: 0.9183 - val_loss: 0.3524 - val_categorical_accuracy: 0.8510\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.83776 to 0.85103, saving model to cnn_weights_best.hdf5\n",
      "Epoch 9/30\n",
      "1947/1947 [==============================] - 2s 792us/step - loss: 0.1404 - categorical_accuracy: 0.9440 - val_loss: 0.3743 - val_categorical_accuracy: 0.8437\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.85103\n",
      "Epoch 10/30\n",
      "1947/1947 [==============================] - 2s 956us/step - loss: 0.1238 - categorical_accuracy: 0.9522 - val_loss: 0.3464 - val_categorical_accuracy: 0.8555\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.85103 to 0.85546, saving model to cnn_weights_best.hdf5\n",
      "Epoch 11/30\n",
      "1947/1947 [==============================] - 1s 737us/step - loss: 0.1007 - categorical_accuracy: 0.9666 - val_loss: 0.3622 - val_categorical_accuracy: 0.8510\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.85546\n",
      "Epoch 12/30\n",
      "1947/1947 [==============================] - 1s 733us/step - loss: 0.0837 - categorical_accuracy: 0.9712 - val_loss: 0.3764 - val_categorical_accuracy: 0.8540\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.85546\n",
      "Epoch 13/30\n",
      "1947/1947 [==============================] - 1s 728us/step - loss: 0.0686 - categorical_accuracy: 0.9764 - val_loss: 0.4286 - val_categorical_accuracy: 0.8437\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.85546\n",
      "Epoch 14/30\n",
      "1947/1947 [==============================] - 1s 730us/step - loss: 0.0615 - categorical_accuracy: 0.9784 - val_loss: 0.3864 - val_categorical_accuracy: 0.8614\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.85546 to 0.86136, saving model to cnn_weights_best.hdf5\n",
      "Epoch 15/30\n",
      "1947/1947 [==============================] - 1s 742us/step - loss: 0.0718 - categorical_accuracy: 0.9789 - val_loss: 0.4468 - val_categorical_accuracy: 0.8422\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.86136\n",
      "Epoch 16/30\n",
      "1947/1947 [==============================] - 1s 742us/step - loss: 0.0785 - categorical_accuracy: 0.9697 - val_loss: 0.4108 - val_categorical_accuracy: 0.8628\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.86136 to 0.86283, saving model to cnn_weights_best.hdf5\n",
      "Epoch 17/30\n",
      "1947/1947 [==============================] - 1s 748us/step - loss: 0.0593 - categorical_accuracy: 0.9779 - val_loss: 0.4880 - val_categorical_accuracy: 0.8481\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.86283\n",
      "Epoch 18/30\n",
      "1947/1947 [==============================] - 1s 746us/step - loss: 0.0453 - categorical_accuracy: 0.9836 - val_loss: 0.4844 - val_categorical_accuracy: 0.8540\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.86283\n",
      "Epoch 19/30\n",
      "1947/1947 [==============================] - 1s 756us/step - loss: 0.0619 - categorical_accuracy: 0.9784 - val_loss: 0.4552 - val_categorical_accuracy: 0.8673\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy improved from 0.86283 to 0.86726, saving model to cnn_weights_best.hdf5\n",
      "Epoch 20/30\n",
      "1947/1947 [==============================] - 1s 731us/step - loss: 0.0422 - categorical_accuracy: 0.9856 - val_loss: 0.4646 - val_categorical_accuracy: 0.8555\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.86726\n",
      "Epoch 21/30\n",
      "1947/1947 [==============================] - 1s 756us/step - loss: 0.0467 - categorical_accuracy: 0.9841 - val_loss: 0.4856 - val_categorical_accuracy: 0.8599\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.86726\n",
      "Epoch 22/30\n",
      "1947/1947 [==============================] - 1s 747us/step - loss: 0.0493 - categorical_accuracy: 0.9815 - val_loss: 0.5061 - val_categorical_accuracy: 0.8555\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.86726\n",
      "Epoch 23/30\n",
      "1947/1947 [==============================] - 1s 754us/step - loss: 0.0472 - categorical_accuracy: 0.9825 - val_loss: 0.5411 - val_categorical_accuracy: 0.8525\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.86726\n",
      "Epoch 24/30\n",
      "1947/1947 [==============================] - 1s 734us/step - loss: 0.0340 - categorical_accuracy: 0.9892 - val_loss: 0.5563 - val_categorical_accuracy: 0.8481\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.86726\n",
      "Epoch 25/30\n",
      "1947/1947 [==============================] - 1s 732us/step - loss: 0.0371 - categorical_accuracy: 0.9872 - val_loss: 0.5198 - val_categorical_accuracy: 0.8702\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy improved from 0.86726 to 0.87021, saving model to cnn_weights_best.hdf5\n",
      "Epoch 26/30\n",
      "1947/1947 [==============================] - 1s 731us/step - loss: 0.0382 - categorical_accuracy: 0.9892 - val_loss: 0.5292 - val_categorical_accuracy: 0.8614\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.87021\n",
      "Epoch 27/30\n",
      "1947/1947 [==============================] - 1s 743us/step - loss: 0.0342 - categorical_accuracy: 0.9887 - val_loss: 0.5365 - val_categorical_accuracy: 0.8673\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.87021\n",
      "Epoch 28/30\n",
      "1947/1947 [==============================] - 1s 729us/step - loss: 0.0536 - categorical_accuracy: 0.9789 - val_loss: 0.6577 - val_categorical_accuracy: 0.8407\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.87021\n",
      "Epoch 29/30\n",
      "1947/1947 [==============================] - 1s 739us/step - loss: 0.0549 - categorical_accuracy: 0.9800 - val_loss: 0.5998 - val_categorical_accuracy: 0.8437\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.87021\n",
      "Epoch 30/30\n",
      "1947/1947 [==============================] - 1s 743us/step - loss: 0.0442 - categorical_accuracy: 0.9851 - val_loss: 0.5888 - val_categorical_accuracy: 0.8422\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.87021\n"
     ]
    }
   ],
   "source": [
    "train(model_cnn,'cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(name, use_meta=False):\n",
    "    model1 = load_model(name+'_weights_best.hdf5')\n",
    "    if use_pos and use_meta:\n",
    "        if use_dep:\n",
    "            preds = model1.predict([X_test,X_test_pos, X_test_dep, X_test_meta], batch_size=batch_size, verbose=1)\n",
    "        else:\n",
    "            preds = model1.predict([X_test,X_test_pos, X_test_meta], batch_size=batch_size, verbose=1)\n",
    "    elif use_meta:\n",
    "        if use_dep:\n",
    "            preds = model1.predict([X_test, X_test_dep, X_test_meta], batch_size=batch_size, verbose=1)\n",
    "        else:\n",
    "            preds = model1.predict([X_test, X_test_meta], batch_size=batch_size, verbose=1)\n",
    "    elif use_pos:\n",
    "        if use_dep:\n",
    "            preds = model1.predict([X_test, X_test_pos, X_test_dep], batch_size=batch_size, verbose=1)\n",
    "        else:\n",
    "            preds = model1.predict([X_test, X_test_pos], batch_size=batch_size, verbose=1)\n",
    "    else:\n",
    "        if use_dep:\n",
    "            preds = model1.predict([X_test, X_test_dep], batch_size=batch_size, verbose=1)\n",
    "        else:\n",
    "            preds = model1.predict([X_test], batch_size=batch_size, verbose=1)\n",
    "    false_worst = {}\n",
    "    true_best = {}\n",
    "    label_list = ['yes','no']\n",
    "    \n",
    "    Y_test_gt = list(test_data['rumor_id'])\n",
    "    predictions = np.array([np.argmax(pred) for pred in preds])\n",
    "    \n",
    "    print(len(predictions)==len(Y_test_gt))\n",
    "    correct = np.sum(predictions == Y_test_gt)\n",
    "    acc = correct*100.0/len(Y_test_gt)\n",
    "    print(\"Correctly Predicted\")\n",
    "    print(\"Accuracy : \")\n",
    "    pickle.dump(predictions, open(name+'_predictions.p','wb'))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/674 [==============================] - 0s 372us/step\n",
      "True\n",
      "Correctly Predicted\n",
      "Accuracy : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.64688427299703"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('cnn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
